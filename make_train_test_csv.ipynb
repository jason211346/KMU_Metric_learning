{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from os import walk\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "# from loss.SoftTriple import SoftTriple\n",
    "from loss.SoftTriple import SoftTriple\n",
    "import evaluation as eva\n",
    "# import net\n",
    "from net.bninception import bninception\n",
    "# from net.resnet_model import ResNet50\n",
    "# from net.resnet_model_cls import ResNet50\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset_triplet_cub import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '/root/notebooks/dataset/CUB_100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/root/notebooks/dataset/CUB_100/train/'\n",
    "test_dir = '/root/notebooks/dataset/CUB_100/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "train = []\n",
    "test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in walk(target_dir):\n",
    "    a = root.split('/')\n",
    "#     print((len(a)))\n",
    "    if len(a) == 7:\n",
    "        if 'train' in a:\n",
    "#             print(a[-1])\n",
    "            labels.append(a[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  (4714, 3)\n"
     ]
    }
   ],
   "source": [
    "for num, label in enumerate(labels):\n",
    "    for file in os.listdir(os.path.join(train_dir, label)):\n",
    "        train.append([train_dir+'{}/{}'.format(label, file), num, label])\n",
    "        \n",
    "train = pd.DataFrame(train, columns=['file_path', 'class', 'label_name'])\n",
    "\n",
    "print('Training Data: ',train.shape)\n",
    "train.to_csv('CUB100_train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Data:  (1150, 3)\n"
     ]
    }
   ],
   "source": [
    "for num, label in enumerate(labels):\n",
    "    for file in os.listdir(os.path.join(test_dir, label)):\n",
    "        test.append([test_dir+'{}/{}'.format(label, file), num, label])\n",
    "        \n",
    "test = pd.DataFrame(test, columns=['file_path', 'class', 'label_name'])\n",
    "\n",
    "print('test Data: ',test.shape)\n",
    "test.to_csv('CUB100_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from os import walk\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "# from loss.SoftTriple import SoftTriple\n",
    "from loss.SoftTriple import SoftTriple\n",
    "import evaluation as eva\n",
    "# import net\n",
    "from net.bninception import bninception\n",
    "# from net.resnet_model import ResNet50\n",
    "# from net.resnet_model_cls import ResNet50\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.dataset_triplet_cub import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '/root/notebooks/dataset/v9/'\n",
    "train_dir = '/root/notebooks/dataset/v9/train/'\n",
    "test_dir = '/root/notebooks/dataset/v9/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "train = []\n",
    "test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in walk(target_dir):\n",
    "#     print(root)\n",
    "    a = root.split('/')\n",
    "#     print((len(a)))\n",
    "    if len(a) == 7:\n",
    "        if 'train' in a:\n",
    "#             print(a[-1])\n",
    "            labels.append(a[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_0', 'a_1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF=['after','before']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BF = os.listdir(train_dir+labels[0]+'/'+KMU_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  (360, 5)\n"
     ]
    }
   ],
   "source": [
    "for num, label in enumerate(labels):\n",
    "    KMU_id = sorted(os.listdir(train_dir+label))\n",
    "    for ID in KMU_id:\n",
    "        for file in sorted(os.listdir(os.path.join(train_dir, label,ID,BF[0]))):\n",
    "#             print(train_dir+'{}/{}/{}/{}'.format(label,ID,BF[0], file), num, label,ID,BF[0])\n",
    "            train.append([train_dir+'{}/{}/{}/{}'.format(label,ID,BF[0], file), num, label,ID,BF[0]])\n",
    "        \n",
    "        \n",
    "train = pd.DataFrame(train, columns=['file_path', 'class', 'label_name','ID','BF'])\n",
    "\n",
    "print('Training Data: ',train.shape)\n",
    "train.to_csv('v9_after_train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data:  (150, 5)\n"
     ]
    }
   ],
   "source": [
    "for num, label in enumerate(labels):\n",
    "    KMU_id = sorted(os.listdir(test_dir+label))\n",
    "    for ID in KMU_id:\n",
    "        for file in sorted(os.listdir(os.path.join(test_dir, label,ID,BF[0]))):\n",
    "#             print(test_dir+'{}/{}'.format(label, file), num, label,ID,BF[0])\n",
    "            test.append([test_dir+'{}/{}/{}/{}'.format(label,ID,BF[0], file), num, label,ID,BF[0]])\n",
    "        \n",
    "        \n",
    "test = pd.DataFrame(test, columns=['file_path', 'class', 'label_name','ID','BF'])\n",
    "\n",
    "print('Testing Data: ',test.shape)\n",
    "test.to_csv('v9_after_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  (360, 5)\n"
     ]
    }
   ],
   "source": [
    "for num, label in enumerate(labels):\n",
    "    KMU_id = sorted(os.listdir(train_dir+label))\n",
    "    for ID in KMU_id:\n",
    "        for file in sorted(os.listdir(os.path.join(train_dir, label,ID,BF[1]))):\n",
    "#             print(train_dir+'{}/{}'.format(label, file), num, label,ID,BF[0])\n",
    "            train.append([train_dir+'{}/{}/{}/{}'.format(label,ID,BF[1], file), num, label,ID,BF[1]])\n",
    "        \n",
    "        \n",
    "train = pd.DataFrame(train, columns=['file_path', 'class', 'label_name','ID','BF'])\n",
    "\n",
    "print('Training Data: ',train.shape)\n",
    "train.to_csv('v9_before_train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data:  (150, 5)\n"
     ]
    }
   ],
   "source": [
    "for num, label in enumerate(labels):\n",
    "    KMU_id = sorted(os.listdir(test_dir+label))\n",
    "    for ID in KMU_id:\n",
    "        for file in sorted(os.listdir(os.path.join(test_dir, label,ID,BF[1]))):\n",
    "#             print(test_dir+'{}/{}'.format(label, file), num, label,ID,BF[0])\n",
    "            test.append([test_dir+'{}/{}/{}/{}'.format(label,ID,BF[1], file), num, label,ID,BF[1]])\n",
    "        \n",
    "        \n",
    "test = pd.DataFrame(test, columns=['file_path', 'class', 'label_name','ID','BF'])\n",
    "\n",
    "print('Testing Data: ',test.shape)\n",
    "test.to_csv('v9_before_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from os import walk\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "# from loss.SoftTriple import SoftTriple\n",
    "from loss.SoftTriple import SoftTriple\n",
    "import evaluation as eva\n",
    "# import net\n",
    "from net.bninception import bninception\n",
    "# from net.resnet_model import ResNet50\n",
    "# from net.resnet_model_cls import ResNet50\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.dataset_triplet_KMU import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transfrom(augment = False):\n",
    "#     normalize = transforms.Normalize(mean=[104., 117., 128.],\n",
    "#                                          std=[1., 1., 1.])    \n",
    "    normalize = transforms.Normalize(mean=[128., 117., 104.],\n",
    "                                         std=[1., 1., 1.])    \n",
    "    if augment:\n",
    "        # If images are to be augmented, add extra operations for it (first two).\n",
    "                \n",
    "        transform = transforms.Compose([\n",
    "#             transforms.Lambda(RGB2BGR),\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255)),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        # If no augmentation is needed then apply only the normalization and resizing operations.\n",
    "        transform = transforms.Compose([\n",
    "#             transforms.Lambda(RGB2BGR),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255)),\n",
    "            normalize,\n",
    "        ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "train_before_csv = '/root/notebooks/KMU_softtriple_loss-main/dataset_csv/v9/v9_before_train_data.csv'\n",
    "train_after_csv = '/root/notebooks/KMU_softtriple_loss-main/dataset_csv/v9/v9_after_train_data.csv'\n",
    "\n",
    "test_before_csv = '/root/notebooks/KMU_softtriple_loss-main/dataset_csv/v9/v9_before_test_data.csv'\n",
    "test_after_csv = '/root/notebooks/KMU_softtriple_loss-main/dataset_csv/v9/v9_after_test_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "# load data\n",
    "train_before_dataframe =  pd.read_csv(train_before_csv)\n",
    "train_after_dataframe = pd.read_csv(train_after_csv)\n",
    "\n",
    "test_before_dataframe = pd.read_csv(test_before_csv)\n",
    "test_after_dataframe = pd.read_csv(test_after_csv)\n",
    "\n",
    "\n",
    "train_transform = Transfrom(augment= True)\n",
    "test_transform = Transfrom(augment= False)\n",
    "\n",
    "\n",
    "train_dataset   = Dataset(train_before_dataframe, train_after_dataframe, train_transform)\n",
    "test_dataset   = Dataset(test_before_dataframe,test_after_dataframe, test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=3,num_workers=0,shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=3,num_workers=0, shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_img_list = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_img_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_img_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_before_csv = '/root/notebooks/KMU_softtriple_loss-main/dataset_csv/v9/v9_before_train_data.csv'\n",
    "train_after_csv = '/root/notebooks/KMU_softtriple_loss-main/dataset_csv/v9/v9_after_train_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_before_dataframe =  pd.read_csv(train_before_csv)\n",
    "train_after_dataframe = pd.read_csv(train_after_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ID = train_after_dataframe[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = img_ID[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFR_B_img_path = train_before_dataframe.loc[(train_before_dataframe['ID']==ID)]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/notebooks/dataset/v9/train/a_0/x_682.jpg',\n",
       " '/root/notebooks/dataset/v9/train/a_0/x_683.jpg',\n",
       " '/root/notebooks/dataset/v9/train/a_0/x_684.jpg']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in LFR_B_img_path:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  pd.read_csv('CUB2_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = random.randint(0,len(a.num))\n",
    "A_label = a.num[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 = random.randint(0,len(a.num))\n",
    "P_label\n",
    "if a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[104., 117., 128.],\n",
    "                                     std=[1., 1., 1.])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose([\n",
    "            transforms.Lambda(RGB2BGR),\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255)),\n",
    "            normalize,\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = 'dataset_csv/CUB2_train_data.csv'\n",
    "test_csv = 'dataset_csv/CUB2_test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB2BGR(im):\n",
    "    assert im.mode == 'RGB'\n",
    "    r, g, b = im.split()\n",
    "    return Image.merge('RGB', (b, g, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Transfrom(augment = False):\n",
    "#     normalize = transforms.Normalize(mean=[104., 117., 128.],\n",
    "#                                          std=[1., 1., 1.])     \n",
    "    if augment:\n",
    "        # If images are to be augmented, add extra operations for it (first two).\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Lambda(RGB2BGR),\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        # If no augmentation is needed then apply only the normalization and resizing operations.\n",
    "        transform = transforms.Compose([\n",
    "\n",
    "            transforms.Lambda(RGB2BGR),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    return transform\n",
    "dataframe =  pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe =  pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transfrom(augment= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = Transfrom(augment= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset   = Dataset(dataframe, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset   = Dataset(test_dataframe, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,num_workers=1,shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,num_workers=4,shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, img2,img3 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, img2,img3 = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, (data1, data2, data3) = next(iter(enumerate(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, (data1, data2, data3) = next(iter(enumerate(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
